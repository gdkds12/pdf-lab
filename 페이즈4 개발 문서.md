# Phase 4 기술 문서 (Reducer / Global Reasoning)
## Project Thunder v3.0 — “Global Context Reasoning” 구현 지침 [file:1]

> Phase 4는 Phase 2~3(Map)에서 수집한 “신호(signals) + 근거 후보(evidence_candidates)”를 통합(Reduce)하여, Long Context 모델로 최종 Exam Report를 생성하는 단계다. [file:1]  
> 이 단계만이 **최종 판정/검증/분류** 권한을 가진다. [file:1]

---

## 1) 목적과 비목적
### 목적
- 세션 단위로 “강의 전체 신호 타임라인 + 관련 교재 청크 묶음”을 조립하고, Long Context 추론으로 최종 Exam Report JSON을 생성한다. [file:1]
- 중복 제거(Deduplication), 근거 매핑(Verification), 최종 분류(professor_mentioned/likely/trap_warnings)를 수행한다. [file:1]

### 비목적(Phase 4에서 하지 말 것)
- Phase 4는 “추가 검색”을 수행하지 않는다(검색은 Phase 3에서 끝난다). [file:1]
- Phase 4는 “근거 없는 항목을 임의로 생성”하지 않는다(근거 매핑 실패 시 제외). [file:1]

---

## 2) 입력(데이터 계약)
Phase 4 워커는 최소한 아래 입력을 조립해 모델에 전달해야 한다.

### DB에서 읽어올 것(세션 단위)
- `signals`: 세션 전체 신호, 타임라인 정렬용 `chunk_index/t0_sec/t1_sec`, 그리고 검색 의도는 이미 Phase 3에서 사용되었으므로 Phase 4에서는 “판정 재료”로만 활용한다. [file:1]
- `evidence_candidates`: 세션에서 수집된 후보군, 그리고 Phase 4에서 `chunk_id` 기준 중복 제거를 수행한다. [file:1]
- `chunks`: 실제 교재 청크 원문(`content_text`)과 메타(`source_id`, `page_start/end`, `anchor_path`)를 로드한다. [file:1]
- (권장) `chunks.token_count`: Phase 4 입력 토큰 예산 제어를 위해 활용한다(없으면 길이/문자수로 대체).  

### 입력 조립 원칙(가장 중요)
- “References를 단순 텍스트 뭉치로 합치면 식별자가 유실되어 citations 생성이 불안정해진다”는 리스크가 있으므로, 각 청크는 **메타 포함 블록 형태**로 모델 입력에 포함한다.  
- 블록 예시(권장 포맷):
  - `[[CHUNK chunk_id=... source_id=... page=45-47 anchor=Chapter2/Gauss]]`
  - 다음 줄부터 해당 `chunks.content_text` 본문

이 규칙은 최종 출력에서 `citations[]`에 `chunk_id/page/anchor_path`를 정확히 재기입하게 만드는 핵심 장치다.

---

## 3) Context Assembly (구현 절차)
아래 순서를 “고정 파이프라인”으로 두면 디버깅과 재현성이 좋아진다.

### Step A. Signals 타임라인 구성
1. `signals`를 `chunk_index ASC, t0_sec ASC`로 정렬한다. [file:1]
2. 각 signal을 다음 형태의 라인으로 직렬화한다(권장).
   - `[#SIGNAL signal_id=... audio_chunk_id=... t=12.3-18.7 type=hint] content=...`
3. “동일/유사 signal 통합”은 **모델의 역할**로 남기되, 서버는 최소한의 정규화(공백 정리, 깨진 유니코드 제거)만 수행한다.

### Step B. Evidence 후보 dedup + 로딩
1. `evidence_candidates`를 `session_id`로 로드한다. [file:1]
2. Phase 4에서 `chunk_id` 기준으로 dedup한다. [file:1]
3. dedup된 `chunk_id` 목록으로 `chunks`에서 본문과 메타를 로드한다. [file:1]
4. dedup 이후에도 너무 길면(토큰 예산 초과), 아래 우선순위로 컷한다(권장).
   - (1) 여러 `signal_id`에서 반복 등장한 chunk
   - (2) `rrf_score` 상위
   - (3) vector+keyword 둘 다 강한 후보(`rank_vector`와 `rank_keyword` 모두 상위권)
   - (4) 그 외

### Step C. 최종 Prompt Package 생성
모델 입력은 크게 3 덩어리로 구성한다(권장).
- (1) Session meta(과목/범위/옵션)
- (2) Signals timeline
- (3) References chunk blocks(메타 포함)

---

## 4) Model Call (Long Context 추론)
### 모델 역할 정의(SSOT)
모델은 “수석 조교(Grand Master)” 역할로, 신호와 근거를 모두 읽고 다음을 수행한다. [file:1]
- 통합(Synthesis): 반복 신호를 합치고 중요도를 올린다. [file:1]
- 검증(Verification): 신호에 맞는 교재 근거를 매핑하고, 근거가 없으면 제외한다. [file:1]
- 작성(Writing): professor_mentioned / likely / trap_warnings JSON을 작성한다. [file:1]

### 출력 강제(권장)
- 모델 출력은 JSON만 반환하도록 강제한다(구조화 출력).
- 서버는 JSON 파싱 실패 시 1회 재시도 후 실패 처리(무한 재시도 금지).
- 출력 JSON은 “추가 설명 텍스트”가 섞이지 않도록 엄격히 검증한다.

---

## 5) 출력 스키마 & 서버 검증(필수 가드레일)
### 최종 출력(고정)
- `professor_mentioned[]`, `likely[]`, `trap_warnings[]` 3배열 JSON으로 고정한다. [file:1]

### ReportItem 최소 필드(권장)
- `title`: 대표 개념/문제명
- `why`: 왜 중요한지(신호 요약 + 근거 요약)
- `confidence`: 0..1
- `audio_refs[]`: 최소 `(audio_chunk_id, t0_sec, t1_sec, signal_id)`
- `citations[]`: 최소 `(source_id, page_start, page_end, anchor_path, chunk_id)`

### Trap 항목(권장 확장)
- `pitfall`, `why_confusing`, `recommended_answer_style`, `correct_answer_core`

### 서버 측 의미 검증(반드시 구현)
아래 케이스는 “형식은 맞지만 의미가 깨진” 출력이므로, 서버가 제거/강등/실패 처리해야 한다.
- `citations[]`가 비어 있는데도 `confidence`가 높음(예: 0.7 이상)
- `audio_refs[]`가 비어 있는데 “교수 명시적 예고(professor_mentioned)”로 분류됨
- `page_start > page_end` 같은 메타 불일치
- `chunk_id`가 실제 `chunks`에 존재하지 않음(모델이 hallucination으로 만든 ID)

### 저장
- 검증을 통과한 최종 JSON을 `session_reports.report_json`에 저장하고, 세션 상태를 `completed`로 변경한다. [file:1]

---

## 구현 체크리스트(요약)
- [ ] Signals 타임라인 직렬화 포맷 고정(재현성/디버깅).
- [ ] References는 반드시 “메타 포함 블록”으로 입력(식별자 유실 방지).
- [ ] dedup은 `chunk_id` 기준, 예산 초과 시 컷 우선순위 규칙 보유.
- [ ] JSON only 출력 강제 + 서버 의미 검증(근거 없는 항목 금지).
- [ ] 성공 시 `session_reports` 저장 + 상태 전이.
