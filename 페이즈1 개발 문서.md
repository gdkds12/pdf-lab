```markdown
# Project Thunder 개발 작업 지시서 (Phase 1)
## Phase 1: Cloud Run Native PDF Ingest Pipeline (v3.0 최신)

> 본 문서는 Project Thunder v3.0의 **교재(PDF) 지식베이스 구축 파이프라인**(Phase 1) 개발 지침이다. [file:1]  
> 핵심 목표는 `GCS URI 입력 → (Router) Digital/Scanned 분기 → (Parse/OCR) 페이지 보존 전사 → Chunking(메타 포함) → Embedding(gemini-embedding-001) → OCI Supabase(chunks) 적재` 이다. [file:1]

---

## 1. 개요 및 목표
Phase 1은 이후 Phase 3 하이브리드 검색과 Phase 4 “메타 포함 블록(citations)”을 성립시키기 위해, **청크에 page/anchor/ID가 정확히 남는 형태**로 교재를 구조화한다. [file:1]  
특히 Scanned PDF는 Gemini를 사용하되, “요약/재배열”로 인해 페이지 매핑이 깨지지 않도록 **전사 정책을 강제**한다. [file:1]

---

## 2. 기술 스택 및 환경
- Runtime: Python 3.10+ (Dockerized, Cloud Run Jobs)  
- Infrastructure:
  - Compute: GCP Cloud Run Jobs (권장: CPU 2+, Memory 4GiB+; 대용량/스캔본은 8GiB 고려)
  - Storage: GCS(Input), Local `/tmp`(ephemeral)
  - AI/ML:
    - Gemini: `gemini-2.5-flash-lite` (OCR/Parsing/구조화 전사) [file:1]
    - Embedding: `gemini-embedding-001` (768d) [file:1]
  - DB: OCI Supabase(Postgres + pgvector/halfvec) → `chunks` 테이블 적재 [file:1]
- Key Libraries(권장):
  - `google-cloud-storage`, `google-cloud-aiplatform`
  - `pymupdf`(fitz)
  - `supabase`
  - `tenacity`
  - (옵션) `tiktoken` 또는 자체 토큰 추정기(Phase 4 예산용 token_count 계산)

---

## 3. 입력/출력 계약(SSOT)
### 입력
- `source_id` (DB에 이미 등록된 교재/노트 소스 ID)
- `gcs_pdf_url` 또는 `gs://bucket/path.pdf`

### 출력
- Supabase `chunks` 테이블에 row N개 삽입/업서트 [file:1]
  - 필수: `source_id`, `content_text`, `embedding(halfvec 768)`, `page_start`, `page_end`, `anchor_path[]` [file:1]
  - 권장: `content_hash`, `token_count` (중복 ingest 방지/예산 제어) [file:1]

---

## 4. 단계별 개발 상세 (Step-by-Step)

### Step 0. Cloud Run Job 스캐폴딩 / Docker
- Base image: `python:3.10-slim`
- 불필요한 OS 패키지 설치 최소화(대부분 PyMuPDF만으로 처리)  
- 환경변수(예시):
  - `GCP_PROJECT`, `GCP_LOCATION`
  - `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`
  - `INGEST_BATCH_PAGES=20` (Scanned 분할 기본값)
  - `EMBED_BATCH_SIZE=8` (임베딩 배치)

**Check**
- 로컬 Docker 빌드/실행, GCS 접근, Supabase 연결이 모두 성공하는지.

---

### Step 1. GCS 다운로드 (메모리 안정성)
- `gs://` URI를 파싱해 bucket/blob 추출
- 다운로드 경로: `/tmp/{uuid}.pdf`
- 처리 후 `finally`에서 무조건 삭제

**Check**
- 100MB+ PDF 다운로드 시 RSS 메모리가 급증하지 않고 `/tmp`로 안정적으로 저장되는지.

---

### Step 2. Router (Digital vs Scanned)
목표: 전체 문서를 한 번에 OCR로 보내지 않고, “필요할 때만” Gemini를 사용하도록 분기한다. [file:1]

**Logic**
1. `fitz.open(local_pdf_path)`
2. 첫 3페이지 `page.get_text("text")` 수행
3. 텍스트 밀도(예: “페이지당 글자수 평균”)가 임계값 미만이면 `SCANNED`, 이상이면 `DIGITAL`

**권장 보강**
- “3페이지 중 2페이지 이상이 임계값 미만” 같은 다수결 룰로 오분류를 줄인다.
- Router 결과를 `sources.ingest_status` 또는 별도 로그 테이블에 남겨 재현성 확보(선택). [file:1]

---

### Step 3A. DIGITAL 트랙: PyMuPDF 텍스트 추출(빠른 경로)
DIGITAL은 **Gemini 미사용**을 기본으로 하여 비용/지연을 최소화한다. [file:1]

**Logic**
- 페이지별로 텍스트를 추출하고, “페이지 번호 보존” 상태로 파이프라인에 넘긴다.
- 출력 포맷(내부 표준, 예시):
```json
{
  "pages": [
    { "page_num": 1, "text": "..." },
    { "page_num": 2, "text": "..." }
  ]
}
```

**주의**
- DIGITAL에서도 header/section 구조(anchor 후보)가 필요하므로, 최소한의 헤더 감지(예: 대문자 라인, 번호 패턴) 또는 “약식 anchor_path=['Page 10']”라도 반드시 채운다(후속 citations 안정성). [file:1]

---

### Step 3B. SCANNED 트랙: PDF 분할 + Gemini OCR/Parsing(핵심)
SCANNED는 “페이지 매핑이 절대 깨지지 않는 전사”가 목표다. [file:1]

#### (1) PDF Splitting (Batching)
- 기본: 20페이지 단위 sub-PDF로 분할 [file:1]
- 구현: `fitz.Document()`에 `insert_pdf()`로 범위 복사 후 `tobytes()`로 bytes 생성
- 각 배치에는 메타를 붙인다:
  - `batch_index`, `page_start`, `page_end` (원본 기준)

**Check**
- 배치 순서가 유지되고(page_start/end), 누락 없이 전체 페이지가 커버되는지.

#### (2) Gemini 호출 (application/pdf 직접 전달)
- `Part.from_data(pdf_bytes, mime_type="application/pdf")` 형태로 전달(이미지 변환 파이프라인 제거)
- Retry: `tenacity`로 429/5xx에 대해 지수 백오프

#### (3) OCR/Parsing 출력 포맷(페이지 보존 강제)
요약/재배열로 매핑이 깨지는 것을 막기 위해, 출력은 “페이지 단위 JSON”으로 고정하는 것을 권장한다. [file:1]

- Gemini에 강제할 내부 출력(JSON) 예시:
```json
{
  "pages": [
    {
      "page_num": 45,
      "markdown": "..."
    }
  ]
}
```

- 전사 정책(프롬프트에 반드시 포함):
  - 누락 금지 / 요약 금지 / 재정렬 금지
  - 표는 Markdown Table, 수식은 LaTeX로 구조 유지
  - 페이지 번호를 정확히 유지(배치 시작 페이지 기준 오프셋 적용)

**Check**
- 수식/표가 구조를 유지하는지, 페이지 번호가 원본과 일치하는지.

---

### Step 4. Chunking (메타 포함, SSOT 준수)
Phase 3/4를 위해 “청크 메타데이터”가 가장 중요하다. [file:1]

#### (1) Anchor 추출(권장)
- 우선순위:
  1. Markdown heading 기반(`##`, `###`)
  2. 번호 패턴(예: `3.2`, `Definition`, `Theorem`)
  3. fallback: `anchor_path=["Page {page_num}"]`

#### (2) Chunk 생성 규칙(권장 기본)
- 1차: heading 단위로 split
- 2차: 길이 초과 시 문단 단위 split
- 각 chunk에는 반드시:
  - `page_start`, `page_end` (최소 동일 페이지라도 채움)
  - `anchor_path[]` (최소 1개 요소)
  - `content_text` (검색/인용 원문)

#### (3) content_hash / token_count (권장)
- `content_hash`: `(source_id + normalized_text)` 해시로 생성(재실행 시 idempotency)
- `token_count`: Phase 4 예산 기반 context assembly를 위한 값(없으면 서버가 추정) [file:1]

**Check**
- chunk가 너무 잘게 쪼개져 검색 품질이 떨어지지 않는지(“의미 단위” 유지).
- page_start/end와 anchor_path가 비어 있는 row가 절대 생성되지 않는지.

---

### Step 5. Embedding (gemini-embedding-001, 768d)
- 모델: `gemini-embedding-001` [file:1]
- 배치: 5~10개 묶음 요청(호출 수 절감)
- 오류 처리: 부분 실패 시 실패한 청크만 재시도(전체 롤백 금지)

**Check**
- embedding 차원이 768로 고정되는지, 빈 문자열이 들어가지 않는지.

---

### Step 6. DB 적재 (OCI Supabase → chunks)
Target: `chunks` 테이블 [file:1]

권장 payload(SSOT v3.0 기준):
```python
{
  "source_id": source_id,
  "content_text": chunk_text,
  "embedding": embedding_vector,    # List[float] (서버/DB에서 halfvec로 캐스팅/저장)
  "page_start": page_start,
  "page_end": page_end,
  "anchor_path": anchor_path,       # List[str]
  "content_hash": content_hash,     # 권장
  "token_count": token_count        # 권장
}
```

**업서트/중복 방지**
- `content_hash` 기반 unique index가 있으면 upsert로 재실행 안정성 확보(권장). [file:1]

**Check**
- Supabase에서 row 조회 가능 + embedding 컬럼 채워짐
- page_start/end, anchor_path가 누락된 row가 없는지

---

## 5. 디버깅 및 검증 시나리오 (Test Plan)
### Scenario A: Digital PDF 처리(빠른 경로)
- 입력: 텍스트 선택 가능한 PDF(논문/전자책)
- 기대:
  - Router: DIGITAL
  - Gemini 미사용(또는 최소화)
  - DB 저장 속도가 빠름 [file:1]

### Scenario B: Scanned PDF 처리(OCR 경로)
- 입력: 드래그 불가 이미지형 PDF(스캔본)
- 기대:
  - Router: SCANNED
  - 20페이지 단위 분할 → Gemini OCR JSON(page 보존) → chunking → DB 저장 [file:1]

### Scenario C: 대용량 안정성(100p+)
- 입력: 100페이지 이상 PDF
- 기대:
  - OOM 없이 완주
  - 배치 단위로 진행되며 중간 실패 시 부분 재시도 가능 [file:1]

---

## 6. 산출물(Definition of Done)
- `thunder-ingest-worker` Cloud Run Job 이미지
- 다음이 포함된 실행 로그:
  - Router 판정(DIGITAL/SCANNED)
  - SCANNED 배치 처리 진행률(page_start/end)
  - chunk 개수, embedding 배치 수, DB 삽입/업서트 건수
- Scenario A/B/C 전부 통과 증적(로그 + Supabase 샘플 row) [file:1]
```
